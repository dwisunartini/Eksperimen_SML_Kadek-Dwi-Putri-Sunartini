# -*- coding: utf-8 -*-
"""automate_kadek-dwi-putri-sunartini.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yIkEFYuppzBoMZ90rvklLsCBtL0ZECqG

# **1. Perkenalan Dataset**

1. **Sumber Dataset**:  
   Dataset dapat diperoleh dari Kaggle. Berikut tautan untuk mengakses dataset: https://www.kaggle.com/datasets/ayeshaseherr/exame-score-dataset
   
2. **Deskripsi Dataset**:

# **2. Import Library**

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
import os

"""# **3. Memuat Dataset**

Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.

Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan
"""

def automate_preprocessing(input_path, output_path):

    if not os.path.exists(input_path):
        print(f"Error: File {input_path} tidak ditemukan!")
        return

#if __name__ == "__main__":
    # SESUAIKAN PATH BERDASARKAN GOOGLE DRIVE ANDA
    #drive_path = '/content/drive/MyDrive/SUBMISSION AKHIR/'
    #input_file = drive_path + 'Exam_Score_Prediction.csv'
    #output_file = drive_path + 'examscore_preprocessing_automate.csv'

    # Memanggil fungsi dengan variabel yang sudah didefinisikan
    #automate_preprocessing(input_file, output_file)

if __name__ == "__main__":
    # Logika Fleksibel: Cek apakah jalan di Colab atau GitHub
    if os.path.exists('/content/drive/MyDrive/SUBMISSION AKHIR/'):
        drive_path = '/content/drive/MyDrive/SUBMISSION AKHIR/'
    else:
        # Jika di GitHub Actions, dataset harus diupload ke repo yang sama
        drive_path = './'

    input_file = os.path.join(drive_path, 'Exam_Score_Prediction.csv')
    output_file = os.path.join(drive_path, 'examscore_preprocessing_automate.csv')

df = pd.read_csv(input_file)

"""# **4. Data Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.

Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:
1. Menghapus atau Menangani Data Kosong (Missing Values)
2. Menghapus Data Duplikat
3. Normalisasi atau Standarisasi Fitur
4. Deteksi dan Penanganan Outlier
5. Encoding Data Kategorikal
6. Binning (Pengelompokan Data)

Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur.
"""

# Data Cleaning (Menghapus ID dan Duplikat)
df = df.drop(columns=['student_id'], errors='ignore').drop_duplicates()

# Menangani Missing Values
for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].fillna(df[col].mode()[0])
        else:
            df[col] = df[col].fillna(df[col].median())

# Penanganan Outlier dengan Metode IQR
numeric_check = ['study_hours', 'sleep_hours', 'class_attendance']
for col in numeric_check:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

# Normalisasi Teks (Pembersihan Spasi dan Huruf Kecil)
    # Langkah krusial agar mapping tidak menghasilkan NaN
for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].astype(str).str.lower().str.strip()

# Encoding Kategorikal Manual
    # Mapping Ordinal & Nominal
df['gender'] = df['gender'].map({'male': 1, 'female': 2, 'other': 0})
df['internet_access'] = df['internet_access'].map({'yes': 1, 'no': 0})
df['sleep_quality'] = df['sleep_quality'].map({'poor': 0, 'average': 1, 'good': 2})

df['study_method'] = df['study_method'].map({
        'self-study': 0, 'group study': 1, 'online videos': 2, 'coaching': 3, 'mixed': 4
    })

df['course'] = df['course'].map({
        'b.com': 0, 'diploma': 1, 'bca': 2, 'b.sc': 3, 'ba': 4, 'bba': 5, 'b.tech': 6
    })

df['facility_rating'] = df['facility_rating'].map({'high': 2, 'medium': 1, 'low': 0})
df['exam_difficulty'] = df['exam_difficulty'].map({'easy': 0, 'moderate': 1, 'hard': 2})

# Standarisasi Fitur (StandardScaler)
scaler = StandardScaler()
target = 'exam_score'
features = [c for c in df.columns if c != target]
df[features] = scaler.fit_transform(df[features])

# Menyimpan hasil prapemrosesan
df.to_csv(output_file, index=False)

print(f"Otomatisasi Berhasil!")
print(f"Data tersimpan di: {output_file}")
print(f"Ukuran data akhir: {df.shape}")
